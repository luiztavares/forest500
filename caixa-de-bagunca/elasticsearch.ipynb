{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import datetime\n",
    "from glob import glob\n",
    "es = Elasticsearch()\n",
    "\n",
    "def parse_date(tweet):\n",
    "    tweet['created_at'] = datetime.datetime.strptime(tweet['created_at'],'%a %b %d %H:%M:%S +0000 %Y')\n",
    "    tweet['user']['created_at'] = datetime.datetime.strptime(tweet['user']['created_at'],'%a %b %d %H:%M:%S +0000 %Y')\n",
    "    if ('retweeted_status' in tweet):\n",
    "        tweet['retweeted_status']['created_at'] = datetime.datetime.strptime(tweet['retweeted_status']['created_at'],'%a %b %d %H:%M:%S +0000 %Y')\n",
    "        tweet['retweeted_status']['user']['created_at'] = datetime.datetime.strptime(tweet['retweeted_status']['user']['created_at'],'%a %b %d %H:%M:%S +0000 %Y')\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest500settings = {\n",
    "    \"settings\":{\n",
    "        \"index\":{\n",
    "            \"mapping\":{\n",
    "                \"total_fields\":{\n",
    "                    \"limit\":\"2000\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "es.indices.create('forest500',body=forest500settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#companies_path = glob('/Users/luiz/OneDrive/forest500/output-company/*.json')\n",
    "companies_path = glob('../data/company/*.json')\n",
    "for company_path in companies_path:\n",
    "    with open(company_path) as f:\n",
    "        tweets = json.load(f)\n",
    "    \n",
    "    print(company_path)\n",
    "\n",
    "    for tweet in tweets:\n",
    "        tweet = parse_date(tweet)\n",
    "        tweet['user']['type'] = 'company'\n",
    "        res = es.index(index=\"forest500\", id=tweet['id'], body=tweet)\n",
    "\n",
    "es.indices.refresh(index=\"forest500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#companies_path = glob('/Users/luiz/OneDrive/forest500/output/*.json')\n",
    "companies_path = glob('../data/if/*.json')\n",
    "for company_path in companies_path:\n",
    "    with open(company_path) as f:\n",
    "        tweets = json.load(f)\n",
    "    \n",
    "    print(company_path)\n",
    "\n",
    "    for tweet in tweets:\n",
    "        tweet = parse_date(tweet)\n",
    "        tweet['user']['type'] = 'financial'\n",
    "        res = es.index(index=\"forest500\", id=tweet['id'], body=tweet)\n",
    "\n",
    "es.indices.refresh(index=\"forest500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "  \"query\": {\n",
    "    \"query_string\": {\n",
    "      \"query\": \"SantanderAM_UK\",\n",
    "      \"default_field\": \"user.screen_name\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "res = es.search(index=\"forest500\", body=query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "signatories = {}\n",
    "companies = {}\n",
    "financial = {}\n",
    "\n",
    "with open('/Users/luiz/OneDrive/forest500/input/list of signatories.csv') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=';')\n",
    "    for row in spamreader:\n",
    "        signatories[row[1]] =  row[2:]\n",
    "\n",
    "with open('/Users/luiz/Downloads/companies.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=';', quotechar='\"')\n",
    "    for row in spamreader:\n",
    "        if(len(row) > 1):\n",
    "            companies[row[0]] = row[1]\n",
    "\n",
    "with open('/Users/luiz/Downloads/financial.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=';', quotechar='\"')\n",
    "    for row in spamreader:\n",
    "        if(len(row) > 1):\n",
    "            financial[row[0]] = row[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3M\n",
      "agdagro\n",
      "adidas\n",
      "adidasfootball\n",
      "adidasoriginals\n",
      "adidasUK\n",
      "adidasrunning\n",
      "adidasUS\n",
      "adidasHoops\n",
      "adidasGolf\n",
      "adidasalerts\n",
      "adidassoccer\n",
      "Adient\n",
      "AdityaBirlaGrp\n",
      "Advance_Digital\n",
      "AEON_JAPAN\n",
      "AFASCL\n",
      "Agrifirm\n",
      "AholdDelhaize\n",
      "AldiUK\n",
      "Allana_Sons\n",
      "havaianaseurope\n",
      "osklen\n",
      "mizunousa \n",
      "amazon\n",
      "AmazonUK\n",
      "AMOREPACIFIC_US\n",
      "Amul_Coop\n",
      "Arauco_cl\n",
      "ADMupdates\n",
      "arcor\n",
      "ArlaFoodsUK\n",
      "AshleyHomeStore\n",
      "ASICSamerica\n",
      "Runkeeper\n",
      "ASICSaustralia\n",
      " ASICSTennis\n",
      "ASICSeurope\n",
      "ASICS_ZA\n",
      "ASICSUK\n",
      "ASICSvolleyball\n",
      "ASICS_India\n",
      "ABFIngredients\n",
      "adeo\n",
      "BASFCorporation\n",
      "BASFAgro\n",
      "BASFAgProducts\n",
      "BASF\n",
      "BataCompanyNZ\n",
      "beidahuanggroup\n",
      "Bertelsmann_com\n",
      "BESTGroupUK\n",
      "BestGroupInt\n",
      "BFG_Logistics\n",
      "BioMarGroup\n",
      "2SFGofficial\n",
      "BRF_Brasil\n",
      "bricapar\n",
      "bright_food\n",
      "bunge\n",
      "clarksshoes\n",
      "Calbee_JP\n",
      "CampbellSoupCo\n",
      "MichaelKors\n",
      " jimmychoo\n",
      " versace\n",
      "cargill\n",
      "CargillEMEA\n",
      "Cargill_K12Food\n",
      "CargillGrainUS\n",
      "CargillAnimalUK\n",
      "foodsecureworld\n",
      "CarrefourSA\n",
      "CarrefourGroup\n",
      "CarrefourUAE\n",
      "CarrefourUAE\n",
      "CarrefourKe\n",
      "Carrefour_qa\n",
      "Groupe_Casino\n",
      "CencosudArg\n",
      "pr_CP\n",
      "MengniuGroup\n",
      "CSCECNEWS\n",
      "clariant\n",
      "ClariantOilSvcs\n",
      "ClariantMB\n",
      "ClariantCDP\n",
      " ClariantOI\n",
      "CloroxCo\n",
      "ImprensaCoamo\n",
      "UK_COFCO\n",
      " COFCOINTL\n",
      "CP_News\n",
      "ColgateSmile\n",
      "Colgate\n",
      "  Colgate UK\n",
      "ColgateUK\n",
      "ConagraBrands\n",
      "Coop_Fernheim\n",
      "corpovex_VE\n",
      "Costco\n",
      " CostcoUK\n",
      "cotyinc\n",
      "cresudir\n",
      "ricyrela\n",
      "danone\n",
      "De_Heus_Voeders\n",
      "dekelagrivision\n",
      "subway\n",
      "dominos  dominos_uk\n",
      "dunkinbrands\n",
      "dupont_news\n",
      "grupoebro\n",
      "eightplc\n",
      "somoscmpc\n",
      "essity\n",
      "esteelauder esteelauderuk\n",
      "felda_global\n",
      "ferrerouk\n",
      "fleurymichon\n",
      "ForFarmers\n",
      "frigoconcepcion\n",
      "fg_buzz\n",
      "gapinc ukgap\n",
      "generalmills\n",
      "Genting_Bet\n",
      "gsk\n",
      "glencore\n",
      "godrejgroup\n",
      "avril\n",
      "groupe_lactalis\n",
      "savencia_groupe\n",
      "grupo_bimbo\n",
      "GruppoMastrotto\n",
      "hmunitedkingdom\n",
      "hain_celestial\n",
      "haritagroup\n",
      "henkel\n",
      "hersheycompany\n",
      "homedepot\n",
      "hormelfoods\n",
      "htoogroup\n",
      "ICAGruppen\n",
      "IKEAUK\n",
      "inditexcareers \n",
      "recreosantafe\n",
      "intlpaperco\n",
      "invivogroup\n",
      "sainsburys\n",
      "jbs_oficial\n",
      "smuckers\n",
      "JNJNews\n",
      "kaoglobal\n",
      "kelloggsuki\n",
      "keringgroup\n",
      "kewpieusa\n",
      "kikkomanuk\n",
      "kccorp\n",
      "kingfisherplc\n",
      "dsm\n",
      "frieslndcampina\n",
      "korindogroup\n",
      "kraftheinzco\n",
      "kroger\n",
      "loreal\n",
      "landolakesinc\n",
      "legouessant\n",
      "learcorporation\n",
      "lentaruofficial\n",
      "lining_official\n",
      "lindt\n",
      "WeAreLDC\n",
      "WeAreLDC\n",
      "lvmh\n",
      "marfrig_mrfg3\n",
      "marksandspencer\n",
      "marsglobal\n",
      "beiersdorf_ag\n",
      "mcdonalds \n",
      "mcdonaldsuk\n",
      "meijiamerica \n",
      "meijicoltd\n",
      "meredithcorp\n",
      "metro_news\n",
      "minerva_beef3\n",
      "mitsubishiuk\n",
      "mitsubishi_usa\n",
      "citra_citrasa\n",
      "mizkan_official\n",
      "mdlz\n",
      "mondigroup\n",
      "mrvoficial\n",
      "musimmas_group\n",
      "naturaandco\n",
      "natuzzi\n",
      "nesteglobal\n",
      "nestle\n",
      "nestleuki\n",
      "newbalanceuk\n",
      " newbalance\n",
      "newscorp\n",
      "thenicegroup\n",
      "nike\n",
      "ndpapermills\n",
      "nisshin_oillio\n",
      "nitoriofficial\n",
      "droetkerbakes\n",
      "olam\n",
      "orklagroup\n",
      "pmifoods\n",
      "pypayurved\n",
      "pearson\n",
      "pepsico\n",
      "lperezcompanc\n",
      "ptpn_xiv\n",
      "pertamina\n",
      "perumperhutani\n",
      "plukonfoodgroup\n",
      "prada\n",
      "preciouswoods\n",
      "proctergamble\n",
      "astraintrnt\n",
      "RNusindo_id\n",
      "rrdonnelley\n",
      "discoverrb\n",
      "relxhq\n",
      "rewe_group\n",
      "shell\n",
      "sadesa_leather\n",
      "saintgobain\n",
      "salim_group\n",
      "recruitmentagro\n",
      "mysamsonite\n",
      "scjohnson\n",
      "kaufland\n",
      "lidlgb\n",
      "sekisuihouseau\n",
      "shiseido_corp\n",
      "shiseido_uk\n",
      "GROUPE_SIFCA\n",
      "simeproperty\n",
      "smart_sinarmas\n",
      "skechers_uk\n",
      " skechersusa\n",
      "Socfinindonesia\n",
      "Sodrugestvospzk\n",
      "sparintheuk\n",
      " sparint\n",
      "staples\n",
      "staplesuk\n",
      "starbucks\n",
      "starbucksuk\n",
      "stevemadden_uk\n",
      "stevemadden\n",
      "storaenso\n",
      "suguna_foods\n",
      "coach\n",
      "katespadeny\n",
      "stuartweitzman\n",
      "target\n",
      "tesco\n",
      "tetrapak \n",
      "thomsonreuters\n",
      "reutersuk\n",
      "tjmaxx\n",
      "marshalls\n",
      "total\n",
      "triputragroup\n",
      "tysonfoods\n",
      "unilever\n",
      "upmglobal\n",
      "vfcorp\n",
      "walmart\n",
      "wendys\n",
      "westrock\n",
      "morrisons\n",
      "wolverineww\n",
      "woolworths_sa\n",
      "woolworths\n",
      "x5_retailgroup\n",
      "yakultph \n",
      "yildiz_holding\n",
      "the_japan_news\n",
      "ypfoficial\n",
      "yumbrands\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "companies = pd.read_excel('../input/forest500.xlsx',sheet_name='company')\n",
    "financial = pd.read_excel('../input/forest500.xlsx',sheet_name='financial')\n",
    "\n",
    "for index,row in companies.iterrows():\n",
    "    if(pd.isna(row['Handles'])):\n",
    "        continue\n",
    "    handles = row['Handles'].split(',')\n",
    "    for handle in handles:\n",
    "        print(handle)\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"term\": {\n",
    "                    \"user.screen_name.keyword\": {\n",
    "                        \"value\": handle,\n",
    "                        \"boost\": 1.0\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"_source\": False\n",
    "        }\n",
    "\n",
    "\n",
    "        res = helpers.scan(\n",
    "                        client = es,\n",
    "                        query = query, \n",
    "                        index = 'forest500')\n",
    "\n",
    "        for tweet in res:\n",
    "            update_query = {\n",
    "                'doc':{\n",
    "                    \n",
    "                        'official_name': handle,\n",
    "                        'type': row['Type'],\n",
    "                        'subsidary': row['Subsidary'],\n",
    "                        'brands': str(row['Brands']).split(','),\n",
    "                        'sectors': str(row['Company sectors']).split(','),\n",
    "                        'headquarters': row['Headquarters'],\n",
    "                        'region': row['HQ region'],\n",
    "                        'commodities_powerbroker': str(row['Commodities (powerbroker_']).split(','),\n",
    "                        'segments_powerbroker': str(row['Segments (powerbroker)']).split(','),\n",
    "                        'commodities_other': str(row['Commodities (Other)']).split(','),\n",
    "                        'segments_other': str(row['Segments (Other)']).split(','),\n",
    "                        'signatories': str(row['Sign']).split(','),\n",
    "                }\n",
    "            }\n",
    "            try:\n",
    "                es.update(index='forest500', id=tweet['_id'], body=update_query)\n",
    "            except:\n",
    "                print(update_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABNAMRO\n",
      "Aegon\n",
      "AMGFunds\n",
      "Allianz\n",
      "Allstate\n",
      "AmericanCentury\n",
      "AmericanEquity\n",
      "amfam\n",
      "AIGinsurance\n",
      "ameriprise\n",
      "AmBankMY\n",
      "AQRCapital\n",
      "ANZ_AU\n",
      "AXA\n",
      "AXA_Partners\n",
      "AXANext\n",
      "axainsurance\n",
      "AXAIM\n",
      "AXAVentures\n",
      "BaillieGifford\n",
      "bbva\n",
      "BancodoBrasil\n",
      "BankofAmerica\n",
      "china_bank\n",
      "BNYMellon\n",
      "Barclays\n",
      "blackrock\n",
      "BlackRock_UK\n",
      " BLKPublicPolicy\n",
      "BlackRockCA\n",
      "BMO\n",
      "bndes\n",
      "bnpparibas\n",
      "GroupeBPCE\n",
      "bradesco\n",
      "calpers\n",
      "capitalgroup\n",
      "charlesschwab\n",
      "citi\n",
      "commerzbank\n",
      "commbank\n",
      "rabobankgroup\n",
      "creditagricole\n",
      "cic\n",
      "creditsuisse\n",
      "dbsbank\n",
      "deutschebank\n",
      "dimensional\n",
      "eatonvance\n",
      "epfoaindia\n",
      "farmcredit\n",
      "farmermacnews\n",
      "fidelity\n",
      "fifththird\n",
      "fti_global\n",
      "gicsingapore \n",
      "goldmansachs\n",
      "GSBSociety\n",
      "guggenheimptnrs\n",
      "homeloansbyhdfc\n",
      "hsbc\n",
      "icicibank\n",
      "eximbank_id\n",
      "ing_news\n",
      "intesasanpaolo\n",
      "InvescoCanada\n",
      "invescoindia\n",
      "InvescoUS\n",
      "invescoemea\n",
      "itau\n",
      "janushenderson\n",
      "jefferies\n",
      "jpmorgan\n",
      "kbank_live\n",
      "kkr_co\n",
      "kotakbankltd\n",
      "Krungthai_Care\n",
      "lbbw\n",
      "lazard\n",
      "landg_group\n",
      "fti_global \n",
      "libertymutual\n",
      "licindiaforever\n",
      "lincolnfingroup\n",
      "lbgplc\n",
      "macquarie\n",
      "Manulife\n",
      "MassMutualUS\n",
      "MetLife\n",
      "MUFGAmericas\n",
      "MizuhoAmericas\n",
      "MorganStanley\n",
      "NAB\n",
      "Nationwide\n",
      "NatWestGroup\n",
      "NewYorkLife\n",
      "Nomura\n",
      "Nordea\n",
      "NorgesBank\n",
      "NorthernTrust\n",
      "NM_News\n",
      "OCBCBank\n",
      "PNCBank\n",
      "principal\n",
      "Prudential\n",
      "prudentialplc\n",
      "mandiricare\n",
      "BNI\n",
      "RBC\n",
      "SafraInvestment\n",
      "bancosantander\n",
      "santanderuk\n",
      "santanderukhelp\n",
      "SantanderBankUS\n",
      "santanderpb\n",
      "SantanderUSA\n",
      "santander_br\n",
      "santanderukbiz\n",
      "SantanderDevs\n",
      "Santander_Ar\n",
      "SantanderAM_UK\n",
      "SantanderMx\n",
      "santander_es\n",
      "SantanderBankPL\n",
      "santanderchile\n",
      "Schroders\n",
      "SchrodersUS\n",
      "SchrodersPR\n",
      "SNB_BNS_en\n",
      "SNB_BNS_de\n",
      "SNB_BNS_it\n",
      "SNB_BNS_fr\n",
      "scotiabank\n",
      "ScotiabankUy\n",
      "ScotiabankApoya\n",
      "ScotiabankMX\n",
      "ScotiabankViews\n",
      "ScotiabankPE\n",
      "ScotiaEconomics\n",
      "ScotiabankGY\n",
      "ScotiabankHelps\n",
      "ScotiabankJM\n",
      "scb_thailand\n",
      "SocieteGenerale\n",
      "SG_PrivateBank\n",
      "SG_etvous\n",
      "SocGen_UK\n",
      "CertSG\n",
      "SocGen_US\n",
      "StanChart\n",
      "StanChartHelp\n",
      "StanChartLive\n",
      "SLA_plc\n",
      "TheOfficialSBI\n",
      "StateFarm\n",
      "StateStreet\n",
      "mitsui_sumitomo\n",
      "SunLife\n",
      "SunLifeUS\n",
      "TRowePrice\n",
      "Temasek\n",
      "TIAA\n",
      "TDBank_US\n",
      "Travelers\n",
      "TruistNews\n",
      "UBS\n",
      "UniCredit_PR\n",
      "USAA\n",
      "USAA_help\n",
      "usbank\n",
      "Vanguard_Group\n",
      "Vanguard_UK\n",
      "wellington_mgmt\n",
      "wellsfargo\n",
      "westernsouthern\n",
      "westpac\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "financial = pd.read_excel('../input/forest500.xlsx',sheet_name='financial')\n",
    "\n",
    "for index,row in financial.iterrows():\n",
    "    if(pd.isna(row['Handles'])):\n",
    "        continue\n",
    "    handles = row['Handles'].split(',')\n",
    "    for handle in handles:\n",
    "        print(handle)\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"term\": {\n",
    "                    \"user.screen_name.keyword\": {\n",
    "                        \"value\": handle,\n",
    "                        \"boost\": 1.0\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"_source\": False\n",
    "        }\n",
    "\n",
    "        res = helpers.scan(\n",
    "                        client = es,\n",
    "                        query = query, \n",
    "                        index = 'forest500')\n",
    "\n",
    "        for tweet in res:\n",
    "            update_query = {\n",
    "                'doc':{\n",
    "                        'official_name': handle,\n",
    "                        'type': row['Type'],\n",
    "                        'subsidary': str(row['FI subsidary']),\n",
    "                        'FI type': str(row['FI type']).split(','),\n",
    "                        'headquarters': str(row['FI Headquarters']),\n",
    "                        'region': str(row['HQ Region']),\n",
    "                        'commodities': str(row['Commodities assessed for']).split(','),\n",
    "                }\n",
    "            }\n",
    "            try:\n",
    "                es.update(index='forest500', id=tweet['_id'], body=update_query)\n",
    "            except:\n",
    "                print(update_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in companies:\n",
    "    handles = companies[company].split(',')\n",
    "    for handle in handles:\n",
    "        handle = handle.replace(' ','').replace('/','').replace('\\n','')\n",
    "        print(handle)\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"term\": {\n",
    "                    \"user.screen_name\": {\n",
    "                        \"value\": handle,\n",
    "                        \"boost\": 1.0\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"_source\": False\n",
    "        }\n",
    "\n",
    "        res = helpers.scan(\n",
    "                        client = es,\n",
    "                        query = query, \n",
    "                        index = 'forest500')\n",
    "\n",
    "        for tweet in res:\n",
    "            update_query = {\n",
    "                'doc':{\n",
    "                    'user':{\n",
    "                        'official_name': company\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            es.update(index='forest500', id=tweet['_id'], body=update_query)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in financial:\n",
    "    if(company == '\\ufeffFI Name'):\n",
    "        continue\n",
    "    handles = financial[company].split(',')\n",
    "    for handle in handles:\n",
    "        handle = handle.replace(' ','').replace('/','').replace('\\n','')\n",
    "        print(handle)\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"term\": {\n",
    "                    \"user.screen_name\": {\n",
    "                        \"value\": handle,\n",
    "                        \"boost\": 1.0\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"_source\": False\n",
    "        }\n",
    "\n",
    "        res = helpers.scan(\n",
    "                        client = es,\n",
    "                        query = query, \n",
    "                        index = 'forest500')\n",
    "\n",
    "        for tweet in res:\n",
    "            update_query = {\n",
    "                'doc':{\n",
    "                    'user':{\n",
    "                        'official_name': company\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            es.update(index='forest500', id=tweet['_id'], body=update_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for signa in signatories:\n",
    "    print(signa)\n",
    "    names = set()\n",
    "\n",
    "    query = {\n",
    "      \"query\": {\n",
    "        \"term\": {\n",
    "          \"user.official_name.keyword\": {\n",
    "            \"value\": signa,\n",
    "            \"case_insensitive\": True\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "        \"fields\": [\n",
    "        \"user.official_name.keyword\"\n",
    "      ],\n",
    "        \"_source\": False\n",
    "    }\n",
    "    \n",
    "    res = helpers.scan(\n",
    "                    client = es,\n",
    "                    query = query, \n",
    "                    index = 'forest500')\n",
    "\n",
    "    for tweet in res:\n",
    "      update_query = {\n",
    "          'doc':{\n",
    "              'user':{\n",
    "                  'signatory': signatories[signa]\n",
    "              }\n",
    "          }\n",
    "      }\n",
    "      names.add(\",\".join(tweet['fields']['user.official_name.keyword']))\n",
    "    es.update(index='forest500', id=tweet['_id'], body=update_query)\n",
    "    if(not len(names)):\n",
    "      print(names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('../input/forest500.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('3.9')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}